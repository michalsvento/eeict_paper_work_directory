 
@TechReport{Pascual2017,
  author     = {Pascual, Santiago and Bonafonte, Antonio and Serr√†, Joan},
  title      = {{SEGAN}: {Speech} {Enhancement} {Generative} {Adversarial} {Network}},
  year       = {2017},
  month      = jun,
  note       = {arXiv:1703.09452 [cs] type: article},
  abstract   = {Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.},
  annote     = {Comment: 5 pages, 4 figures, accepted in INTERSPEECH 2017},
  doi        = {10.48550/arXiv.1703.09452},
  file       = {:Pascual2017 - SEGAN_ Speech Enhancement Generative Adversarial Network.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound},
  school     = {arXiv},
  shorttitle = {{SEGAN}},
  url        = {http://arxiv.org/abs/1703.09452},
  urldate    = {2023-02-22},
}

 
@TechReport{Chan2016,
  author     = {Chan, Stanley H. and Wang, Xiran and Elgendy, Omar A.},
  title      = {Plug-and-{Play} {ADMM} for {Image} {Restoration}: {Fixed} {Point} {Convergence} and {Applications}},
  year       = {2016},
  month      = nov,
  note       = {arXiv:1605.01710 [cs] type: article},
  abstract   = {Alternating direction method of multiplier (ADMM) is a widely used algorithm for solving constrained optimization problems in image restoration. Among many useful features, one critical feature of the ADMM algorithm is its modular structure which allows one to plug in any off-the-shelf image denoising algorithm for a subproblem in the ADMM algorithm. Because of the plug-in nature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM". Plug-and-Play ADMM has demonstrated promising empirical results in a number of recent papers. However, it is unclear under what conditions and by using what denoising algorithms would it guarantee convergence. Also, since Plug-and-Play ADMM uses a specific way to split the variables, it is unclear if fast implementation can be made for common Gaussian and Poissonian image restoration problems. In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed point convergence. We show that for any denoising algorithm satisfying an asymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to a fixed point under a continuation scheme. We also present fast implementations for two image restoration problems on super-resolution and single-photon imaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each problem type, and demonstrate promising experimental results of the algorithm.},
  annote     = {Comment: 14 pages},
  doi        = {10.48550/arXiv.1605.01710},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1605.01710.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  school     = {arXiv},
  shorttitle = {Plug-and-{Play} {ADMM} for {Image} {Restoration}},
  url        = {http://arxiv.org/abs/1605.01710},
  urldate    = {2023-02-22},
}

@Comment{jabref-meta: databaseType:bibtex;}
