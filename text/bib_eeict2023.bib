 
@TechReport{Pascual2017,
  author     = {Pascual, Santiago and Bonafonte, Antonio and Serrà, Joan},
  title      = {{SEGAN}: {Speech} {Enhancement} {Generative} {Adversarial} {Network}},
  year       = {2017},
  month      = jun,
  note       = {arXiv:1703.09452 [cs] type: article},
  abstract   = {Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.},
  annote     = {Comment: 5 pages, 4 figures, accepted in INTERSPEECH 2017},
  doi        = {10.48550/arXiv.1703.09452},
  file       = {:Pascual2017 - SEGAN_ Speech Enhancement Generative Adversarial Network.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound},
  school     = {arXiv},
  shorttitle = {{SEGAN}},
  url        = {http://arxiv.org/abs/1703.09452},
  urldate    = {2023-02-22},
}

 
@TechReport{Chan2016,
  author     = {Chan, Stanley H. and Wang, Xiran and Elgendy, Omar A.},
  title      = {Plug-and-{Play} {ADMM} for {Image} {Restoration}: {Fixed} {Point} {Convergence} and {Applications}},
  year       = {2016},
  month      = nov,
  note       = {arXiv:1605.01710 [cs] type: article},
  abstract   = {Alternating direction method of multiplier (ADMM) is a widely used algorithm for solving constrained optimization problems in image restoration. Among many useful features, one critical feature of the ADMM algorithm is its modular structure which allows one to plug in any off-the-shelf image denoising algorithm for a subproblem in the ADMM algorithm. Because of the plug-in nature, this type of ADMM algorithms is coined the name "Plug-and-Play ADMM". Plug-and-Play ADMM has demonstrated promising empirical results in a number of recent papers. However, it is unclear under what conditions and by using what denoising algorithms would it guarantee convergence. Also, since Plug-and-Play ADMM uses a specific way to split the variables, it is unclear if fast implementation can be made for common Gaussian and Poissonian image restoration problems. In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed point convergence. We show that for any denoising algorithm satisfying an asymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to a fixed point under a continuation scheme. We also present fast implementations for two image restoration problems on super-resolution and single-photon imaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each problem type, and demonstrate promising experimental results of the algorithm.},
  annote     = {Comment: 14 pages},
  doi        = {10.48550/arXiv.1605.01710},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1605.01710.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  school     = {arXiv},
  shorttitle = {Plug-and-{Play} {ADMM} for {Image} {Restoration}},
  url        = {http://arxiv.org/abs/1605.01710},
  urldate    = {2023-02-22},
}

 
@Article{Mokry2020,
  author     = {Mokrý, Ondřej and Rajmic, Pavel},
  journal    = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title      = {Audio {Inpainting}: {Revisited} and {Reweighted}},
  year       = {2020},
  issn       = {2329-9304},
  pages      = {2906--2918},
  volume     = {28},
  abstract   = {In this article, we deal with the problem of sparsity-based audio inpainting, i.e. filling in the missing segments of audio. A consequence of the approaches based on mathematical optimization is the insufficient amplitude of the signal in the filled gaps. Remaining in the framework based on sparsity and convex optimization, we propose improvements to audio inpainting, aiming at compensating for such an energy loss. The new ideas are based on different types of weighting, both in the coefficient and the time domains. We show that our propositions improve the inpainting performance in terms of both the SNR and ODG.},
  doi        = {10.1109/TASLP.2020.3030486},
  file       = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9222235&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzkyMjIyMzU=:application/pdf},
  keywords   = {Transforms, Reliability, Speech processing, Microsoft Windows, Analytical models, Minimization, Time-domain analysis, Audio inpainting, sparse representations, proximal algorithms, Douglas–Rachford algorithm, Chambolle–Pock algorithm, energy loss compensation, amplitude drop},
  shorttitle = {Audio {Inpainting}},
}

 
@Article{Janssen1986,
  author   = {Janssen, A. and Veldhuis, R. and Vries, L.},
  journal  = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  title    = {Adaptive interpolation of discrete-time signals that can be modeled as autoregressive processes},
  year     = {1986},
  issn     = {0096-3518},
  month    = apr,
  number   = {2},
  pages    = {317--330},
  volume   = {34},
  abstract = {This paper presents an adaptive algorithm for the restoration of lost sample values in discrete-time signals that can locally be described by means of autoregressive processes. The only restrictions are that the positions of the unknown samples should be known and that they should be embedded in a sufficiently large neighborhood of known samples. The estimates of the unknown samples are obtained by minimizing the sum of squares of the residual errors that involve estimates of the autoregressive parameters. A statistical analysis shows that, for a burst of lost samples, the expected quadratic interpolation error per sample converges to the signal variance when the burst length tends to infinity. The method is in fact the first step of an iterative algorithm, in which in each iteration step the current estimates of the missing samples are used to compute the new estimates. Furthermore, the feasibility of implementation in hardware for real-time use is established. The method has been tested on artificially generated auto-regressive processes as well as on digitized music and speech signals.},
  doi      = {10.1109/TASSP.1986.1164824},
  file     = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=1164824&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzExNjQ4MjQ=:application/pdf},
  keywords = {Interpolation, Signal processing, Autoregressive processes, Adaptive algorithm, Signal restoration, Statistical analysis, H infinity control, Iterative algorithms, Hardware, Testing},
}

 
@Article{Etter1996,
  author   = {Etter, W.},
  journal  = {IEEE Transactions on Signal Processing},
  title    = {Restoration of a discrete-time signal segment by interpolation based on the left-sided and right-sided autoregressive parameters},
  year     = {1996},
  issn     = {1941-0476},
  month    = may,
  number   = {5},
  pages    = {1124--1135},
  volume   = {44},
  abstract = {This paper presents an algorithm for the interpolation of a missing signal segment on the assumption that the signal can be modeled as an autoregressive (AR) process. Unlike previous algorithms, the presented algorithm does not model the signal of the missing segment and the neighboring signal portions by a single AR-parameter vector. Instead, two separate vectors are used so that stationarity need no longer be assumed to extend beyond both sides of the missing segment. The relaxation of this stationarity assumption is essential when the duration of the missing segment is on the order of the short-time stationarity duration of the signal. The algorithm provides the optimal solution to the problem of interpolating a missing segment based on the left-sided and right-sided AR-parameter vectors. The solution is optimal in the sense of a least-squares residual. The algorithm is applied to speech and music signals and is compared with other restoration techniques.},
  doi      = {10.1109/78.502326},
  file     = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=502326&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50LzUwMjMyNg==:application/pdf},
  keywords = {Signal restoration, Interpolation, Signal processing, Speech, Extrapolation, Iterative algorithms, Multiple signal classification, Sampling methods, Audio tapes, Audio recording},
}

 
@InProceedings{Mokry2019,
  author    = {Mokrý, Ondřej and Záviška, Pavel and Rajmic, Pavel and Veselý, Vítězslav},
  booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  title     = {Introducing {SPAIN} ({SParse} {Audio} {INpainter})},
  year      = {2019},
  month     = sep,
  note      = {ISSN: 2076-1465},
  pages     = {1--5},
  abstract  = {A novel sparsity-based algorithm for audio inpainting is proposed. It is an adaptation of the SPADE algorithm by Kitić et al., originally developed for audio declipping, to the task of audio inpainting. The new SPAIN (SParse Audio INpainter) comes in synthesis and analysis variants. Experiments show that both A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms. Moreover, A-SPAIN performs on a par with the state-of-the-art method based on linear prediction in terms of the SNR, and, for larger gaps, SPAIN is even slightly better in terms of the PEMO-Q psychoacoustic criterion.},
  doi       = {10.23919/EUSIPCO.2019.8902560},
  file      = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8902560&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg5MDI1NjA=:application/pdf},
  issn      = {2076-1465},
  keywords  = {Task analysis, Signal processing algorithms, Approximation algorithms, Signal to noise ratio, Reliability, Time-domain analysis, Inpainting, Sparse, Cosparse, Synthesis, Analysis},
}

 
@Article{Adler2012,
  author   = {Adler, Amir and Emiya, Valentin and Jafari, Maria and Elad, Michael and Gribonval, Remi and Plumbley, Mark},
  journal  = {IEEE Transactions on Audio Speech and Language Processing},
  title    = {Audio {Inpainting}},
  year     = {2012},
  month    = mar,
  pages    = {922--932},
  volume   = {20},
  abstract = {We propose the audio inpainting framework that recovers portions of audio data distorted due to impairments such as impulsive noise, clipping, and packet loss. In this framework, the distorted data are treated as missing and their location is assumed to be known. The signal is decomposed into overlapping time-domain frames and the restoration problem is then formulated as an inverse problem per audio frame. Sparse representation modeling is employed per frame, and each inverse problem is solved using the Orthogonal Matching Pursuit algorithm together with a discrete cosine or a Gabor dictionary. The Signal-to-Noise Ratio performance of this algorithm is shown to be comparable or better than state-of-the-art methods when blocks of samples of variable durations are missing. We also demonstrate that the size of the block of missing samples, rather than the overall number of missing samples, is a crucial parameter for high quality signal restoration. We further introduce a constrained Matching Pursuit approach for the special case of audio declipping that exploits the sign pattern of clipped audio samples and their maximal absolute value, as well as allowing the user to specify the maximum amplitude of the signal. This approach is shown to outperform state-of-the-art and commercially available methods for audio declipping in terms of Signal-to-Noise Ratio.},
  doi      = {10.1109/TASL.2011.2168211},
  file     = {Full Text PDF:https\://www.researchgate.net/profile/Amir-Adler/publication/224258540_Audio_Inpainting/links/55df72c808ae6abe6e8652dc/Audio-Inpainting.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/224258540_Audio_Inpainting:},
}

 
@InProceedings{Zaviska2019,
  author    = {Záviška, Pavel and Rajmic, Pavel and Mokrý, Ondřej and Průša, Zdeněk},
  booktitle = {2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  title     = {A {Proper} {Version} of {Synthesis}-based {Sparse} {Audio} {Declipper}},
  year      = {2019},
  month     = may,
  note      = {ISSN: 2379-190X},
  pages     = {591--595},
  abstract  = {Methods based on sparse representation have found great use in the recovery of audio signals degraded by clipping. The state of the art in declipping within the sparsity-based approaches has been achieved by the SPADE algorithm by Kitić et. al. (LVA/ICA'15). Our recent study (LVA/ICA'18) has shown that although the original S-SPADE can be improved such that it converges faster than the A-SPADE, the restoration quality is significantly worse. In the present paper, we propose a new version of S-SPADE. Experiments show that the novel version of S-SPADE outperforms its old version in terms of restoration quality, and that it is comparable with the A-SPADE while being even slightly faster than A-SPADE.},
  doi       = {10.1109/ICASSP.2019.8682348},
  file      = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8682348&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzg2ODIzNDg=:application/pdf},
  issn      = {2379-190X},
  keywords  = {Convex functions, Time-domain analysis, Discrete Fourier transforms, Minimization, Approximation algorithms, Redundancy, Declipping, Sparse, Cosparse, Synthesis, Analysis},
}

 
@TechReport{Hao2021,
  author     = {Hao, Xiang and Su, Xiangdong and Horaud, Radu and Li, Xiaofei},
  title      = {{FullSubNet}: {A} {Full}-{Band} and {Sub}-{Band} {Fusion} {Model} for {Real}-{Time} {Single}-{Channel} {Speech} {Enhancement}},
  year       = {2021},
  month      = jan,
  note       = {arXiv:2010.15508 [cs, eess] type: article},
  abstract   = {This paper proposes a full-band and sub-band fusion model, named as FullSubNet, for single-channel real-time speech enhancement. Full-band and sub-band refer to the models that input full-band and sub-band noisy spectral feature, output full-band and sub-band speech target, respectively. The sub-band model processes each frequency independently. Its input consists of one frequency and several context frequencies. The output is the prediction of the clean speech target for the corresponding frequency. These two types of models have distinct characteristics. The full-band model can capture the global spectral context and the long-distance cross-band dependencies. However, it lacks the ability to modeling signal stationarity and attending the local spectral pattern. The sub-band model is just the opposite. In our proposed FullSubNet, we connect a pure full-band model and a pure sub-band model sequentially and use practical joint training to integrate these two types of models' advantages. We conducted experiments on the DNS challenge (INTERSPEECH 2020) dataset to evaluate the proposed method. Experimental results show that full-band and sub-band information are complementary, and the FullSubNet can effectively integrate them. Besides, the performance of the FullSubNet also exceeds that of the top-ranked methods in the DNS Challenge (INTERSPEECH 2020).},
  annote     = {Comment: 5 pages, submitted to 2021 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2021)},
  doi        = {10.48550/arXiv.2010.15508},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2010.15508.pdf:application/pdf},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, Electrical Engineering and Systems Science - Signal Processing},
  school     = {arXiv},
  shorttitle = {{FullSubNet}},
  url        = {http://arxiv.org/abs/2010.15508},
  urldate    = {2023-02-27},
}

 
@TechReport{Xia2020,
  author   = {Xia, Yangyang and Braun, Sebastian and Reddy, Chandan K. A. and Dubey, Harishchandra and Cutler, Ross and Tashev, Ivan},
  title    = {Weighted {Speech} {Distortion} {Losses} for {Neural}-network-based {Real}-time {Speech} {Enhancement}},
  year     = {2020},
  month    = feb,
  note     = {arXiv:2001.10601 [cs, eess] type: article},
  abstract = {This paper investigates several aspects of training a RNN (recurrent neural network) that impact the objective and subjective quality of enhanced speech for real-time single-channel speech enhancement. Specifically, we focus on a RNN that enhances short-time speech spectra on a single-frame-in, single-frame-out basis, a framework adopted by most classical signal processing methods. We propose two novel mean-squared-error-based learning objectives that enable separate control over the importance of speech distortion versus noise reduction. The proposed loss functions are evaluated by widely accepted objective quality and intelligibility measures and compared to other competitive online methods. In addition, we study the impact of feature normalization and varying batch sequence lengths on the objective quality of enhanced speech. Finally, we show subjective ratings for the proposed approach and a state-of-the-art real-time RNN-based method.},
  doi      = {10.48550/arXiv.2001.10601},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2001.10601.pdf:application/pdf},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2001.10601},
  urldate  = {2023-02-27},
}

 
@Article{Mokry2021,
  author   = {Mokrý, Ondřej and Rajmic, Pavel},
  journal  = {Signal Processing},
  title    = {Approximal operator with application to audio inpainting},
  year     = {2021},
  issn     = {0165-1684},
  month    = feb,
  pages    = {107807},
  volume   = {179},
  abstract = {In their recent evaluation of time-frequency representations and structured sparsity approaches to audio inpainting, Lieb and Stark (2018) have used a particular mapping as a proximal operator. This operator serves as the fundamental part of an iterative numerical solver. However, their mapping is improperly justified. The present article proves that their mapping is indeed a proximal operator, and also derives its proper counterpart. Furthermore, it is rationalized that Lieb and Stark’s operator can be understood as an approximation of the proper mapping. Surprisingly, in most cases, such an approximation (referred to as the approximal operator) is shown to provide even better numerical results in audio inpainting compared to its proper counterpart, while being computationally much more effective.},
  doi      = {10.1016/j.sigpro.2020.107807},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0165168420303510/pdfft?md5=82adb89ddc72eaa72a97eab0111b08c1&pid=1-s2.0-S0165168420303510-main.pdf&isDTMRedir=Y:application/pdf},
  keywords = {Proximal operator, Proximal algorithms, Approximation, Sparsity, Audio inpainting},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0165168420303510},
  urldate  = {2023-03-05},
}

@InProceedings{Zaviska2021,
  author    = {Z{\'a}vi{\v s}ka, Pavel and Rajmic, Pavel and Mokr{\'y}, Ond{\v{r}}ej},
  booktitle = {2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Audio Dequantization Using (Co)Sparse (Non)Convex Methods},
  year      = {2021},
  address   = {Toronto, Canada},
  month     = june,
  pages     = {701-705},
  doi       = {10.1109/ICASSP39728.2021.9414637},
  issn      = {2379-190X},
}

@InProceedings{Mokry202021,
  author    = {Ond\v{r}ej Mokr\'y and Pavel Rajmic and Pavel Z\'avi\v{s}ka},
  booktitle = {Proceedings of the 23rd International Conference on Digital Audio Effects (DAFx2020)},
  title     = {Flexible framework for audio reconstruction},
  year      = {2020-21},
  address   = {Vienna, Austria},
  month     = sep,
  volume    = {1},
  issn      = {2413-6689},
  url       = {https://dafx2020.mdw.ac.at/proceedings/index.html},
}

@Article{Kowalski2013,
  author    = {Kowalski, M. and Siedenburg, K. and D\"orfler, M.},
  journal   = {IEEE Transactions on Signal Processing},
  title     = {Social Sparsity! Neighborhood Systems Enrich Structured Shrinkage Operators},
  year      = {2013},
  issn      = {1053-587X},
  number    = {10},
  pages     = {2498-2511},
  volume    = {61},
  doi       = {10.1109/TSP.2013.2250967},
  keywords  = {dictionaries;iterative methods;signal reconstruction;Elitist Lasso;coefficient domain modeling;convex functional;dictionary;generalize Group-Lasso;generalized shrinkage operator;iterative thresholding algorithm;sparse signal expansion;structured signal expansion;structured significance map identification;structured thresholding;Convex optimization;iterative thresholding;structured sparsity},
  owner     = {vaclav},
  timestamp = {2014.02.14},
}

@InProceedings{Kitic2015,
  author    = {Kiti{\'c}, Sr{\dj}an and Bertin, Nancy and Gribonval, R{\'e}mi},
  booktitle = {{LVA/ICA 2015} -- The 12th International Conference on Latent Variable Analysis and Signal Separation},
  title     = {Sparsity and cosparsity for audio declipping: a flexible non-convex approach},
  year      = {2015},
  address   = {Liberec, Czech Republic},
  month     = aug,
  pages     = {243--250},
  keywords  = {sparse ; cosparse ; Clipping ; audio ; non-convex ; real-time},
  owner     = {Pavel},
  timestamp = {2016.03.09},
}

@Misc{Gaultier2017,
  author        = {Gaultier, Cl\'{e}ment and Bertin, Nancy and Kiti\'{c}, Sr{\dj}an and Gribonval, R\'{e}mi},
  title         = {A modeling and algorithmic framework for (non)social (co)sparse audio restoration},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1711.11259},
  primaryclass  = {cs.SD},
}

@InProceedings{Tanaka2022,
  author    = {Tomoro Tanaka and Kohei Yatabe and Masahiro Yasuda and Yasuhiro Oikawa},
  booktitle = {2022 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  title     = {{APPLADE}: Adjustable Plug-and-Play Audio Declipper Combining {DNN} with Sparse Optimization},
  year      = {2022},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icassp43922.2022.9747089},
}

@Article{Combettes2011,
  author    = {Combettes, P.L. and Pesquet, J.C.},
  journal   = {Fixed-Point Algorithms for Inverse Problems in Science and Engineering},
  title     = {Proximal splitting methods in signal processing},
  year      = {2011},
  pages     = {185--212},
  volume    = {49},
  doi       = {10.1007/978-1-4419-9569-8_10},
  owner     = {vaclav},
  publisher = {Springer},
  timestamp = {2014.02.14},
}

 
@Article{ValentiniBotinhao2017,
  author    = {Valentini-Botinhao, Cassia},
  journal   = {http://parole.loria.fr/DEMAND/},
  title     = {Noisy speech database for training speech enhancement algorithms and {TTS} models},
  year      = {2017},
  month     = aug,
  abstract  = {Clean and noisy parallel speech database. The database was designed to train and test speech enhancement methods that operate at 48kHz.  A more detailed description can be found in the papers associated with the database. For the 28 speaker dataset, details can be found in: C. Valentini-Botinhao, X. Wang, S. Takaki \& J. Yamagishi, "Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System using Deep Recurrent Neural Networks", In Proc. Interspeech 2016. For the 56 speaker dataset: C. Valentini-Botinhao, X. Wang, S. Takaki \& J. Yamagishi,  "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech”, In Proc. SSW 2016. Some of the noises used to create the noisy speech were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the CSTR VCTK Corpus, available here: https://doi.org/10.7488/ds/1994. The speech-shaped and babble noise files that were used to create this dataset are available here: http://homepages.inf.ed.ac.uk/cvbotinh/se/noises/.},
  copyright = {Creative Commons Attribution 4.0 International Public License},
  doi       = {10.7488/ds/2117},
  file      = {Full Text PDF:https\://datashare.ed.ac.uk/bitstream/10283/2791/7/noisy_trainset_56spk_wav.zip:application/pdf},
  language  = {eng},
  publisher = {University of Edinburgh. School of Informatics. Centre for Speech Technology Research (CSTR)},
  url       = {https://datashare.ed.ac.uk/handle/10283/2791},
  urldate   = {2023-03-10},
}

 
@InProceedings{Rix2001,
  author    = {Rix, A.W. and Beerends, J.G. and Hollier, M.P. and Hekstra, A.P.},
  booktitle = {2001 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{01CH37221})},
  title     = {Perceptual evaluation of speech quality ({PESQ})-a new method for speech quality assessment of telephone networks and codecs},
  year      = {2001},
  month     = may,
  note      = {ISSN: 1520-6149},
  pages     = {749--752 vol.2},
  volume    = {2},
  abstract  = {Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.},
  doi       = {10.1109/ICASSP.2001.941023},
  issn      = {1520-6149},
  keywords  = {Speech analysis, Quality assessment, Distortion measurement, Nonlinear distortion, Nonlinear filters, Telephony, Signal processing, Delay effects, Speech codecs, Degradation},
}

 
@InProceedings{Taal2010,
  author    = {Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
  booktitle = {2010 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
  title     = {A short-time objective intelligibility measure for time-frequency weighted noisy speech},
  year      = {2010},
  month     = mar,
  note      = {ISSN: 2379-190X},
  pages     = {4214--4217},
  abstract  = {Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.},
  doi       = {10.1109/ICASSP.2010.5495701},
  issn      = {2379-190X},
  keywords  = {Weight measurement, Time frequency analysis, Speech processing, Speech enhancement, Degradation, Artificial intelligence, Noise reduction, Signal processing, Noise measurement, Testing, intelligibility prediction, speech enhancement, noisy speech},
}

@Comment{jabref-meta: databaseType:bibtex;}
