\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Audio restoration using plug-and-play approach\\
%	{}
%\thanks{Identify applicable funding agency here. If none, delete this.}
%}

\title{Joint audio denoising and inpainting with plug-and-play proximal algorithm}

\author{\IEEEauthorblockN{Michal Švento}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{Brno University of Technology}\\
%Brno, Czech Republic \\
%212584@vut.cz}
\IEEEauthorblockA{
	\textit{Brno University of Technology, FEEC,} \\
	\textit{Department of Telecommunications,} \\
	Technická 12, 616 00 Brno, Czech Republic \\
	\href{mailto:212584@vut.cz}{212584@vut.cz}
}
\and
\IEEEauthorblockN{Ondřej Mokrý}
%\IEEEauthorblockA{\textit{Signal Processing Laboratory} \\
%\textit{Brno University of Technology}\\
%Brno, Czech Republic \\
%xmokry12@vut.cz}
\IEEEauthorblockA{
	\textit{Brno University of Technology, FEEC,} \\
	\textit{Department of Telecommunications,} \\
	Technická 12, 616 00 Brno, Czech Republic \\
	\href{mailto:xmokry12@vut.cz}{xmokry12@vut.cz}
}
}

\maketitle

\begin{abstract}
\todo{Plug-and-play má potenciál fungovat, přidání nějaké přibližné informace o některých vzorcích (to je zvýšení toho proj faktoru) v některých metrikách zlepšuje kvalitu a citlivost na tento parametr je výrazně nižší než u konvenční metody.}
\end{abstract}

\begin{IEEEkeywords}
speech enhancement, deep learning, denoising, Douglas--Rachford algorithm, inpainting
\end{IEEEkeywords}

\section{Introduction}

Audio enhancement tasks mostly face problems like missing or damaged samples, noise, or clipping.
Considering speech signals, we %should not avoid the intelligibility problems.
are not only interested in restoring the degradation sample by sample, but we also aim at improving the intelligibility of the recorded speech.
Each restoration problem has developed its own way of enhancing the signal.
Nowadays, the best way to differentiate algorithms is into two categories:
conventional, e.g.\ using autoregressive (AR) modeling or sparsity-based optimization, and solutions using deep learning.
The present paper focuses on the case of restoring a partially observed signal whose observed samples are further degraded by noise, i.e., the aim is to perform simultaneous inpainting and denoising of the speech signal.

In conventional approaches to inpainting, the AR-based Janssen \cite{Janssen1986} and Etter~\cite{Etter1996} algorithms dominate in terms of reconstruction quality.
% These approaches are based on autoregressive signal modeling \cite{Mokry2020}.
% Sparse signal representation has changed efficiency of restoration, mainly because increase of computing power.
A more recent, successful class of methods is based on sparsity.
The key idea is that after performing proper time-frequency analysis of an audio signal,
most of the information is concentrated in a few coefficients, i.e., it is sparse.
This can be applied as fitting the sparsest possible reconstruction either to the reliable observed samples \cite{Adler2012, Kitic2015, Zaviska2019, Mokry2019}, or to a signal not much diverging from the observation in the case of denoising \cite{Kowalski2013}.
% The information hidden in frequency representation (using proper time-frequency analysis) is sparse, i.e. we do not need each spectral coefficient to repair the signal with improved subjective results.
%The most advanced works using sparsity are \cite{Adler2012,Kitic2015,Zaviska2019, Mokry2019}.

Deep learning algorithms have also made their own progress in this area.
The most efficient neural network models are autoencoders,
recurrent neural networks (RNN) and
Generative Adversial Networks (GAN).
Current state-of-the-art deep learned algorithms are Speech Enhancement GAN (SEGAN) \cite{Pascual2017}, NSNet \cite{Xia2020}, FullSubNet \cite{Hao2021}.
While learning-based algorithms allow to adapt to real-world signals, rather than to rely on hand-crafted priors like sparsity or the AR nature of signals, they need large datasets for training.
Furthermore, neural networks are usually trained for a specific problem, lacking universal applicability on similar restoration tasks, in contrast to sparsity-based methods \cite{Gaultier2017, Mokry202021, Zaviska2021}.

As a compromise between the conventional and learning-based methods, 
% In \cite{Chan2016} was introduced Plug-and-Play method for image restoration.
the Plug-and-Play method for image restoration was introduced in \cite{Chan2016},
where part of each iteration of an optimization algorithm is replaced by a (learned) denoiser.
% The idea of a hybrid model,
% combining conventional approach (convex minimization) with deep learning,
% has shown succesful.
In the present paper, we propose a hybrid algorithm based on the same paradigm, aiming at restoration of degraded speech.
While \cite{Chan2016} focused on adapting the Alternating Direction method of Multipliers (ADMM) and a recent declipping approach used the learned element only partially \cite{Tanaka2022}, we choose an opposite approach by working with a simple Douglas--Rachford algorithm (DRA) and exploring the trade-off between data fitting and denoising in the algorithm.


%Our motivation is to transform this model to audio problems with minor differences.
%We replace Alternating Direction Multiplier Method (ADMM) with Douglas-Rachford algorithm (DR~algorithm).
%Denoiser will be chosen from state-of-the-art audio denoisers. 

%% Introduction to sections.
The paper is organized as follows. In section \ref{sec:prereq} we introduce the task from mathematical point of view and we define the restoration as a minimization task.
Section \ref{sec:plugaandplay} presents the Plug-and-Play method and its challenges.
Section \ref{sec:eval} discusses the results and further improvements of algorithm.
Finally, Section \ref{sec:conclusion} concludes the paper.

\section{Prerequsities}\label{sec:prereq} 

In this section, we formulate the first task -- inpainting.
The proposed method \cite{Chan2016} assumes any damage,
but we start with missing samples and then expand the model for various damages.
The rest of the section explains minimization problem solved by DR algorithm.
Solution has two approaches.
First, using frequency coefficients as input explained in \ref{subsec:freqcoef}~\cite{Mokry2020}.
Second, using samples in time domain is described in subsection \ref{subsec:timecoef} \cite{Mokry2021}.


\subsection{Task formulation}

We consider column vector $ \mathbf{s} \in \mathbb{R}^{N} $ as our observed damaged single-channel signal of length $ N $.
We have set $ I $ of sample indices $ \{1,2,\dots,N\} $, which have two subsets: $ I^M $ for missing positions and $ I^R $ stands for reliable positions.
Therefore, samples $ \mathbf{s}(I^M) $ are considered reliable (undamaged) and $ \mathbf{s}(I^R) $ are samples, which we are looking for.
It is common to rewrite it in matrix form:

\begin{equation*}
	\mathbf{s}_{\mathrm{R}} = \mathbf{M}_{\mathrm{R}}\mathbf{s},
\end{equation*}
where $  M_{\mathrm{R}} $ is mask matrix $\mathbf{M}_{R} \in \mathbb{R} ^ { |I^R| \times N}$ ($ |I^R|$ as length of subset), selecting rows frow indentity matrix $ \mathbf{M} $\cite{Adler2012}.




\subsection{COEFFS INPUT Aprroach -- WORK NAME}\label{subsec:freqcoef}

We define our task as sparsity-based problem, minimizing $ \ell_0 $ norm of signal $ \mathbf{s} $.
However, this task leads to an NP-hard problem and is hardly solvable \cite{Mokry2020}.
The closest redefinition is to use the $ \ell_1 $ norm as follows:

\begin{equation*}
 \mathop {\operatorname{arg \, min}}_\mathbf {c}\Vert \mathbf {c}\Vert _1 \quad \text{s.t.}\ D\mathbf {c}\in \Gamma, \tag{1}
\end{equation*} 
where $ D $ is synthesis operator (inverse discrete Gabor transform) and we assume $ A $, $ D = A^* $ and therefore reconstructed signal corresponds to $ \mathbf {y} \approx  D\mathbf {c}$.
Set $ \Gamma $ is defined as follows:

\begin{equation*} \Gamma = \lbrace \mathbf {y}\in \mathbb {R}^L\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace, \tag{2}\end{equation*}

One of the suitable solutions is DR algorithm in \ref{alg:DRA_c} \cite{Mokry2020}.

\begin{algorithm}
	\caption{Douglas-Rachford algorithm -- model with frequency coefficients}
	\begin{algorithmic}[1]\label{alg:DRA_c}
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE  $ \gamma > 0 $, $ \delta  \in (0,1)$ ,
		
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{c}}_n=\operatorname{proj}_{\Gamma}(\mathbf{c}_n) $ 
		\STATE $ \mathbf{c}_{n+1} = \mathbf{c}_n + \lambda \left( \operatorname{soft}_{\gamma}\left(2\mathbf{\widetilde{c}}_n-\mathbf{c}_n \right)-\mathbf{\widetilde{c}}_n\right)$
		\ENDFOR
		\RETURN $D(\operatorname{proj}_{\Gamma}(\mathbf{c}_n))$ 
	\end{algorithmic} 
\end{algorithm}
Operator $ \operatorname{proj}_{\Gamma}(arg)$ is projection onto convex set $ \Gamma $ and $\operatorname{soft}_{\gamma}(arg)$ is soft thresholding operator.
Both are proximal operators.

The condition   $ \delta  \in (0,1)$ is strict for convergence of solution \cite{Combettes2010}.


\subsection{SAMPLES as INPUT APPROACH}\label{subsec:timecoef}

Second approach uses time domain samples as input.
It has mainly computational advice against first approach, in algorithm are less time-frequency transformations (e.g less multiplication operations per iteration).
Problem is, 
that we do not know proximal operator of $ \ell_1 $ norm after analysis, but we can use approximal operator \cite{Mokry2021}.

The main minimazation task reformulates as:
\begin{equation*}
	\mathop {\operatorname{arg \, min}}_\mathbf {s}\Vert \mathbf {s}\Vert _1 \quad \text{s.t.}\ \mathbf {s}\in \Gamma, \tag{3}
\end{equation*} 
and $ \Gamma $ is similarly as in  (eqref2) % Tags???????  
\begin{equation*}
	\Gamma = \lbrace \mathbf {s}\in \mathbb {R}^N\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace. \tag{4} 
\end{equation*}
Afterwards our algorithm resolves to following: 
\begin{algorithm}
	\caption{Douglas-Rachford algorithm -- model with time coefficients}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $ \lambda > 0 $, $ \gamma>0 $, $ \mathbf{x}_0 \in \mathbb{R}^{N} $
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{x}}_n=\operatorname{proj}_{\Gamma}(\mathbf{x}_n) $ 
		\STATE $ \mathbf{x}_{n+1} = \mathbf{x}_n + \lambda \left( D\left(\operatorname{soft}_{\gamma}\left(A\left(2\mathbf{\widetilde{x}}_n-\mathbf{x}_n\right) \right)\right) -\mathbf{\widetilde{x}}_n\right)$
		\ENDFOR
		\RETURN $\operatorname{proj}_{\Gamma}(\mathbf{x}_n)$ 
	\end{algorithmic} 
\end{algorithm}

Projection, in this case, is simply replacing reconstructed samples in positions considered reliable.

\section{Plug-and-Play inpainting} \label{sec:plugaandplay}

\subsection{general algorithm}

\begin{algorithm}
	\caption{Plug-and-Play DR algorithm}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE in
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{x}}_n=\operatorname{proj}_{\Gamma}(\mathbf{x}_n) $ 
		\STATE $ \mathbf{x}_{n+1} = \mathbf{x}_n + \lambda \left( \mathcal{D} \left(2\mathbf{\widetilde{x}}_n-\mathbf{x}_n \right)-\mathbf{\widetilde{x}}_n\right)$
		\ENDFOR
		\RETURN $\operatorname{proj}_{\Gamma}(\mathbf{x}_n)$ 
	\end{algorithmic} 
\end{algorithm}
\subsection{choice of denoiser}

\subsection{Denoisers}

\section{Testing data and evaluation}\label{sec:eval}


\section{Conclusion}
\label{sec:conclusion}

\todo{Naše metoda je super (až na to, že nefunguje, ale to nebudeme říkat).}

\todo{Dalším postupem bude testovat použití na jiné související úlohy dle principu plug-and-play (declipping, dekvantizace). Nabízí se zkusit nepoužívat off-the-shelf denoiser, ale vlastní, který je učený na datech příbuzných s naší úlohou (např.\ specifický šum vzniklý výpadkem jistého procenta vzorků). Taktéž by se nabízel transfer learning, tj.\ vzít dobrý denoiser a jenom ho doučit, aby byl schopný redukovat artefakty vzniklé v inpaintovací úloze.}

%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{IEEEtran}
\bibliography{bib_eeict2023}



\end{document}
