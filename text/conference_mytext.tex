\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Audio restoration using plug-and-play approach\\
%	{}
%\thanks{Identify applicable funding agency here. If none, delete this.}
%}

\title{Joint audio denoising and inpainting with plug-and-play proximal algorithm}

\author{\IEEEauthorblockN{Michal Švento}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{Brno University of Technology}\\
%Brno, Czech Republic \\
%212584@vut.cz}
\IEEEauthorblockA{
	\textit{Brno University of Technology, FEEC,} \\
	\textit{Department of Telecommunications,} \\
	Technická 12, 616 00 Brno, Czech Republic \\
	\href{mailto:212584@vut.cz}{212584@vut.cz}
}
\and
\IEEEauthorblockN{Ondřej Mokrý}
%\IEEEauthorblockA{\textit{Signal Processing Laboratory} \\
%\textit{Brno University of Technology}\\
%Brno, Czech Republic \\
%xmokry12@vut.cz}
\IEEEauthorblockA{
	\textit{Brno University of Technology, FEEC,} \\
	\textit{Department of Telecommunications,} \\
	Technická 12, 616 00 Brno, Czech Republic \\
	\href{mailto:xmokry12@vut.cz}{xmokry12@vut.cz}
}
}

\maketitle

\begin{abstract}
\todo{Plug-and-play má potenciál fungovat, přidání nějaké přibližné informace o některých vzorcích (to je zvýšení toho proj faktoru) v některých metrikách zlepšuje kvalitu a citlivost na tento parametr je výrazně nižší než u konvenční metody.}
\end{abstract}

\begin{IEEEkeywords}
speech enhancement, deep learning, denoising, Douglas--Rachford algorithm, inpainting
\end{IEEEkeywords}

\section{Introduction}

Audio enhancement tasks mostly face problems like missing or damaged samples, noise, or clipping.
Considering speech signals, we %should not avoid the intelligibility problems.
are not only interested in restoring the degradation sample by sample, but we also aim at improving the intelligibility of the recorded speech.
Each restoration problem has developed its own way of enhancing the signal.
Nowadays, the best way to differentiate algorithms is into two categories:
conventional, e.g.\ using autoregressive (AR) modeling or sparsity-based optimization, and solutions using deep learning.
The present paper focuses on the case of restoring a partially observed signal whose observed samples are further degraded by noise, i.e., the aim is to perform simultaneous inpainting and denoising of the speech signal.

In conventional approaches to inpainting, the AR-based Janssen \cite{Janssen1986} and Etter~\cite{Etter1996} algorithms dominate in terms of reconstruction quality.
% These approaches are based on autoregressive signal modeling \cite{Mokry2020}.
% Sparse signal representation has changed efficiency of restoration, mainly because increase of computing power.
A more recent, successful class of methods is based on sparsity.
The key idea is that after performing proper time-frequency analysis of an audio signal,
most of the information is concentrated in a few coefficients, i.e., it is sparse.
This can be applied as fitting the sparsest possible reconstruction either to the reliable observed samples \cite{Adler2012, Kitic2015, Zaviska2019, Mokry2019}, or to a signal not much diverging from the observation in the case of denoising \cite{Kowalski2013}.
% The information hidden in frequency representation (using proper time-frequency analysis) is sparse, i.e. we do not need each spectral coefficient to repair the signal with improved subjective results.
%The most advanced works using sparsity are \cite{Adler2012,Kitic2015,Zaviska2019, Mokry2019}.

Deep learning algorithms have also made their own progress in this area.
The most efficient neural network models are autoencoders,
recurrent neural networks (RNN) and
Generative Adversial Networks (GAN).
Current state-of-the-art deep learned algorithms are Speech Enhancement GAN (SEGAN) \cite{Pascual2017}, NSNet \cite{Xia2020}, FullSubNet \cite{Hao2021}.
While learning-based algorithms allow to adapt to real-world signals, rather than to rely on hand-crafted priors like sparsity or the AR nature of signals, they need large datasets for training.
Furthermore, neural networks are usually trained for a specific problem, lacking universal applicability on similar restoration tasks, in contrast to sparsity-based methods \cite{Gaultier2017, Mokry202021, Zaviska2021}.

As a compromise between the conventional and learning-based methods, 
% In \cite{Chan2016} was introduced Plug-and-Play method for image restoration.
the Plug-and-Play method for image restoration was introduced in \cite{Chan2016},
where part of each iteration of an optimization algorithm is replaced by a (learned) denoiser.
% The idea of a hybrid model,
% combining conventional approach (convex minimization) with deep learning,
% has shown succesful.
In the present paper, we propose a hybrid algorithm based on the same paradigm, aiming at restoration of degraded speech.
While \cite{Chan2016} focused on adapting the Alternating Direction method of Multipliers (ADMM) and a recent declipping approach used the learned element only partially \cite{Tanaka2022}, we choose an opposite approach by working with a simple Douglas--Rachford algorithm (DRA) and exploring the trade-off between data fitting and denoising in the algorithm.


%Our motivation is to transform this model to audio problems with minor differences.
%We replace Alternating Direction Multiplier Method (ADMM) with Douglas-Rachford algorithm (DR~algorithm).
%Denoiser will be chosen from state-of-the-art audio denoisers. 

%% Introduction to sections.
The paper is organized as follows. In section \ref{sec:prereq} we introduce the task from mathematical point of view and we define the restoration as a minimization task.
Section \ref{sec:plugaandplay} presents the Plug-and-Play method and its challenges.
Section \ref{sec:eval} discusses the results and further improvements of algorithm.
Finally, Section \ref{sec:conclusion} concludes the paper.

\section{Prerequsities}\label{sec:prereq} 

%\todo{Asi můžeme nechat obě formulace, protože zapadají do příběhu -- máme formulaci pro koeficienty, která umožňuje formulovat DRA s projekcí a soft. My ale chceme upravovat proximální operátor definovaný na signálu, proto máme druhou formulaci a k ní příslušný (přibližný) algoritmus \ref{alg:DRA_t}.
%U varianty koeficienty bych ale možná nechal formulaci, ale algoritmus \ref{alg:DRA_c} bych možná vypustil.}

In this section, we formalize the task of inpainting and denoising and propose algorithmic solutions based on DRA.
% The proposed method \cite{Chan2016} assumes any damage,
% but we start with missing samples and then expand the model for various damages.
% The rest of the section explains minimization problem solved by DRA.
% Solution has two approaches.
First, using frequency coefficients as input explained in \ref{subsec:freqcoef}~\cite{Mokry2020}.
Second, using samples in time domain is described in subsection \ref{subsec:timecoef} \cite{Mokry2021}.


\subsection{Task formulation}

We consider column vector $ \mathbf{s} \in \mathbb{R}^{N} $ as our observed damaged single-channel signal of length $ N $.
We have set $ I $ of sample indices $ \{1,2,\dots,N\} $, which has two disjunctive subsets: $ I^M $ for missing positions and $ I^R $ stands for reliable positions.
Usually, samples $ \mathbf{s}(I^R) $ are considered reliable (undamaged) and $ \mathbf{s}(I^M) $ are samples, which we are looking for.
It is common to rewrite it in matrix form:
\begin{equation*}
	\mathbf{s}_{\mathrm{R}} = \mathbf{M}_{\mathrm{R}}\mathbf{s},
\end{equation*}
where $\mathbf{M}_{R} \in \mathbb{R} ^ { |I^R| \times N}$ ($ |I^R|$ denotes the number of indices in the subset $I^R$) is the mask matrix, selecting rows from indentity matrix corresponding to the indices in $I^R$ \cite{Adler2012}.
In words, $\mathbf{M}_{\mathrm{R}}\mathbf{s}$ represents choosing the samples from $\mathbf{s}$ on positions $I^R$.

It is natural to define the set of signals fitting the observation as
\begin{equation}
	\label{eq:Gamma}
	\Gamma = \lbrace \mathbf{x}\in \mathbb {R}^L\mid \mathbf{M}_{\mathrm{R}}\mathbf{x}=\mathbf{M}_{\mathrm{R}}\mathbf {s}\rbrace.
\end{equation}
In the noise-less case, we would search for a suitable signal in $\Gamma$.
On the other hand, when the observed samples are distorted by noise, we only require the solution $\mathbf{x}$ to be close to the set $\Gamma$.
%\todo{V algoritmech máme proměnnou $\mathbf{x}$ pro signály, tudíž bych ji asi používal už odsud namísto $\mathbf{y}$.}

\subsection{Synthesis-based formulation}\label{subsec:freqcoef}
%\todo{Pro tento přístup se používá označení syntetizující (\textit{synthesis} nebo \textit{synthesis-based formulation}), pro ten v další části analyzující (\textit{analysis}).}

We define our task as of finding a suitable signal in (or close to) $\Gamma$ as a sparsity-based problem, minimizing $ \ell_0 $-norm of the Gabor coefficients.
%\todo{(možná hned používat Gabor coefficients, ať dále dává smysl termín Gabor transform)} coefficients of the signal $ \mathbf{s} $.
However, this task leads to an NP-hard problem and is hardly solvable \cite{Mokry2020}.
The closest redefinition is to use the $ \ell_1 $ norm as follows:

\begin{equation}
	\label{eq:synthesis.gamma}
	\mathop {\operatorname{arg \, min}}_\mathbf {c}\Vert \mathbf {c}\Vert _1 \quad \text{s.t.}\ D\mathbf {c}\in \Gamma
\end{equation} 
where $\mathbf{D} $ is synthesis operator (inverse discrete Gabor transform) and we assume $ \mathbf{A} $, $ \mathbf{D} = \mathbf{A}^* $ and therefore the reconstructed signal corresponds to $ \mathbf {y} =  \mathbf{D}\mathbf {c}$.
\todo{Je správně $ \mathbf {y} \approx \mathbf{D}\mathbf {c}$, nebo $ \mathbf {y} =  \mathbf{D}\mathbf {c}$? : MŠ: myslím, že rovná sa je správne.}
%\todo{Dal bych asi i $A$ a $\mathbf{D}$ jako matice, i když není nutno.}
% Set $ \Gamma $ is defined as follows:

%\begin{equation}
%	\label{eq:Gamma}
%	\Gamma = \lbrace \mathbf {y}\in \mathbb {R}^L\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace,
%\end{equation}

%One of the suitable solutions is DR algorithm in \ref{alg:DRA_c} \cite{Mokry2020}.
%
%\begin{algorithm}
%	\caption{Douglas-Rachford algorithm -- model with frequency coefficients}
%	\begin{algorithmic}[1]\label{alg:DRA_c}
%		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%		\renewcommand{\algorithmicensure}{\textbf{Output:}}
%		\REQUIRE  $ \gamma > 0 $, $ \delta  \in (0,1)$ ,
%		
%		\FOR {$n = 0, 1, \dots$}
%		\STATE $\mathbf{\widetilde{c}}_n=\operatorname{proj}_{\Gamma}(\mathbf{c}_n) $ 
%		\STATE $ \mathbf{c}_{n+1} = \mathbf{c}_n + \lambda \left( \operatorname{soft}_{\gamma}\left(2\mathbf{\widetilde{c}}_n-\mathbf{c}_n \right)-\mathbf{\widetilde{c}}_n\right)$
%		\ENDFOR
%		\RETURN $D(\operatorname{proj}_{\Gamma}(\mathbf{c}_n))$ 
%	\end{algorithmic} 
%\end{algorithm}

In the presence of noise, the condition $D\mathbf {c}\in \Gamma$ is not beneficial, since it forces the solution to still contain the noise.
Its reduction can be included in the formulation by rather minimizing the distance of the solution from the set $\Gamma$.
This can be computed as the difference of the solution $D\mathbb{c}$ and the observation $\mathbf{s}$  on the positions $I^R$, typically, squared $\ell_2$ norm is used in this context:
\begin{equation}
	\label{eq:synthesis.dist}
	\mathop {\operatorname{arg \, min}}_\mathbf {c}\Vert \mathbf {c}\Vert _1 + \frac{\alpha}{2} \Vert \mathbf{M}_{\mathrm{R}} D\mathbf {c} - \mathbf{M}_{\mathrm{R}} \mathbf{s} \Vert^2_2.
\end{equation} 
The parameter $\alpha > 0$ manages the trade-off between sparsity of the coefficients and the data-fidelity.

%Operator $ \operatorname{proj}_{\Gamma}(arg)$ is projection onto convex set $ \Gamma $ and $\operatorname{soft}_{\gamma}(arg)$ is soft thresholding operator.
%Both are proximal operators.
%
%The condition   $ \delta  \in (0,1)$ is strict for convergence of solution \cite{Combettes2011}.
Both \eqref{eq:synthesis.gamma} and \eqref{eq:synthesis.dist} could be solved using DRA \cite{Mokry2020, Zaviska2021}.
However, the algorithms use the Gabor coefficients as the main variable, which makes the incorporation of a denoiser (working with a signal as the input) problematic.


\subsection{Analysis-based formulation}\label{subsec:timecoef}

Second approach uses time domain samples as input.
It has mainly computational advice against first approach, in algorithm are less time-frequency transformations (e.g less multiplication operations per iteration).
Problem is, 
that we do not know proximal operator of $ \ell_1 $ norm after analysis, but we can use approximal operator \cite{Mokry2021}.

The main minimazation task \eqref{eq:analysis.gamma} is reformulated as,
\begin{equation}
	\label{eq:analysis.gamma}
	\mathop {\operatorname{arg \, min}}_\mathbf {x}\Vert \mathbf{A} \mathbf {x}\Vert _1 \quad \text{s.t.}\ \mathbf {x}\in \Gamma,
\end{equation}
or, in the denoising case,
\begin{equation}
	\label{eq:analysis.dist}
	\mathop {\operatorname{arg \, min}}_\mathbf {x}\Vert \mathbf{A} \mathbf {y}\Vert _1 + \frac{\alpha}{2} \Vert \mathbf{M}_{\mathrm{R}} \mathbf {x} - \mathbf{M}_{\mathrm{R}} \mathbf{s} \Vert^2_2.
\end{equation} 
%and $ \Gamma $ is similarly as in \eqref{eq:Gamma} % Tags???????  
%\todo{Tuto rovnici tu máme dvakrát.}
%\begin{equation}
%	\Gamma = \lbrace \mathbf {s}\in \mathbb {R}^N\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace.
%\end{equation}
To solve \eqref{eq:analysis.gamma} or \eqref{eq:analysis.dist}, DRA can also be employed, even though it solves only an approximation of the objective \cite{Mokry2021}.
The algorithm is summarized in Alg.\,\ref{alg:DRA}.

% Afterwards our algorithm resolves to following \cite{Mokry2020}: 
\begin{algorithm}
	\caption{DRA for \eqref{eq:analysis.gamma} or \eqref{eq:analysis.dist}.}
	\begin{algorithmic}[1]\label{alg:DRA}
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $ \lambda_n > 0 $, $ \gamma>0 $, $ \mathbf{\widetilde{x}}_0 \in \mathbb{R}^{N} $, $\beta \in [0, 1]$
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{x}_n= (1-\beta)\mathbf{\widetilde{x}}_n + \beta \operatorname{proj}_{\Gamma}(\mathbf{\widetilde{x}}_n) $ 
		\STATE $ \mathbf{\widetilde{x}}_{n+1} = \mathbf{x}_n + \lambda_n \left( \mathbf{D}\left(\operatorname{soft}_{\gamma}\left(\mathbf{A}\left(2\mathbf{x}_n-\mathbf{\widetilde{x}}_n\right) \right)\right) -\mathbf{x}_n\right)$
		\ENDFOR
		\RETURN $\mathbf{x}_n$ %\todo{DONE:Možná vyměnit značení $\mathbf{\widetilde{x}}$ a $\mathbf{x}$?}
	\end{algorithmic} 
\end{algorithm}

The operator $ \operatorname{proj}_{\Gamma}$ is projection onto convex set $ \Gamma $ and $\operatorname{soft}_{\gamma}$ is soft thresholding operator \cite{Combettes2011}.
Projection, in this case, means replacing reconstructed samples in positions considered reliable by the observed samples.
The parameter $\beta$ allows to differentiate between simple inpainting \eqref{eq:analysis.gamma} and the joint problem \eqref{eq:analysis.dist}.
The case of $\beta = 1$ corresponds to performing projection in the update, in line with \cite{Mokry2020}.
For $\beta < 1$, it can be derived (see e.g.\,\cite[Sec.\,4 and Tab.\,1]{Combettes2011}) that the update corresponds to the so-called proximal operator of $f(\mathbf{x}) = \frac{\alpha}{2} \Vert \mathbf{M}_{\mathrm{R}} \mathbf {x} - \mathbf{M}_{\mathrm{R}} \mathbf{s} \Vert^2_2$ with $\alpha = \frac{\beta}{1-\beta}$.


\section{Plug-and-Play inpainting} \label{sec:plugaandplay}

The Plug-and-Play method was proposed in paper \cite{Chan2016} on image data.
The main idea is replace proximal operator with denoiser without any demands from minimizing algorithm (the origin of Plug-and-Play).
We rewrite this task to solve our audio inpainting problem using DRA with approximal operator (analysis-based) \ref{alg:DRA}.
We reformulate the algorithm in \ref{alg:pnp}:


\begin{algorithm}
	\caption{Plug-and-Play DRA}
	\begin{algorithmic}[1]\label{alg:pnp}
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $ \lambda_n > 0 $, $ \gamma>0 $, $ \mathbf{\widetilde{x}}_0 \in \mathbb{R}^{N} $, $\beta \in [0, 1]$
		\FOR {$n = 0, 1, \dots$}
		\STATE %$\mathbf{\widetilde{x}}_n=\operatorname{proj}_{\Gamma}(\mathbf{x}_n) $ 
		$\mathbf{x}_n= (1-\beta)\mathbf{\widetilde{x}}_n + \beta \operatorname{proj}_{\Gamma}(\mathbf{\widetilde{x}}_n) $ 
		\STATE $ \mathbf{\widetilde{x}}_{n+1} = \mathbf{\widetilde{x}}_n + \lambda_n \left( \mathcal{D} \left(2\mathbf{x}_n-\mathbf{\widetilde{x}}_n \right)-\mathbf{x}_n\right)$
		\ENDFOR
		\RETURN $\mathbf{x}_n$ 
	\end{algorithmic} 
\end{algorithm}

\subsection{Denoisers}

The requirements for our denoiser $\mathcal{D}$ are primarily good results in subjective metrics.
They will be discussed more in \ref{subsec:metrics}.
Implementation of our denoiser is based on mayavoz toolkit \cite{Shahul2023}.
This tool provides us simple use of learned model with downloadable checkpoints.
Our selection is WaveUnet learned on Valentini dateset \cite{ValentiniBotinhao2017}(variant with 28 speakers).

\section{Testing data and evaluation}\label{sec:eval}

As was introduced before, we use discrete Gabor transform as analysis operator $\mathbf{A}$ with following setup parameters: Gauss  window with length $w =1024 $, hop length $a = 256$ and number of frequency channels used in Fast Fourier transform (FFT) $n_{\textup{FFT}} = 1024$.
To satisfy the Parseval tight frame condition, the synthesis operator $\mathbf{D}$ is configured the same \cite{Mokry2020}.

\subsection{Metrics}\label{subsec:metrics}

%\todo{SNR,PESQ,STOI}

A standard metric for reconstruction quality is the signal-to-noise ratio (SNR), where the \textit{signal} represents the clean ground truth and \textit{noise} is the difference of the reconstruction and the ground truth.
We calculate SNR either for whole signal or only on $I^M$, i.e, on the indices of missing samples.
The higher SNR means better reconstruction.

A drawback of SNR is that it measures sample-wise difference, which does not take into account human perception of sound quality.
Valuable metrics for speech signals are those, which measure human perception.
We use two objective metrics: Perceptual Evaluation of Speech Quality (PESQ) \cite{Rix2001} and 
Short-Time Objective Intelligibility (STOI) \cite{Taal2010}.
PESQ has scale from $-0.5$ to $4.5$, with higher score meaning better result.
STOI measures correlation of two signals on a scale from $0$ to $1$.

\subsection{Choice of soft threshold parameter}\label{subsec:soft_thresh}

For reduction of variable parameters in comparison test between conventional and deep learning approach, we decided to fix the $\gamma$ in DRA.
We tried DRA with multiple $\gamma$: $\{0.001, 0.01,0.1\}$.
Results are shown in \ref{fig:gammatest}.
%\todo{OM: graf výsledků např.\ na základě PESQ s komentářem, že ostatní metriky dávají v principu stejný závěr}

\begin{figure}[h]
	\includegraphics[width=1\linewidth]{figures/gamma_test}
	\caption{PESQ metric results made to choose the best $\gamma$ in DRA for testing with multiple audio files.
	The results in another metrics (STOI,SNR) are similar.
	Label \textit{proj fraction} refers to ratio $/beta$ from \ref{alg:DRA}.}
	\label{fig:gammatest}
\end{figure}

The best results were obtain with $\gamma=0.01$, which has shown succesful mostly with ascending ratio of projected signal.

\subsection{Dataset and test setup}
We choose randomly 10 audio files for final evaluation from Valentini dataset \cite{ValentiniBotinhao2017} used commonly in speech enhancement challenges (dataset is split to two subsets: clean and noisy).
We use clean sample as reference and noise as input to algorithm (clean with addition of noise, which is required to supress).
As a simulation of inpainting task, we use uniformly distributed mask of zero and ones, where partition of 0 is $40\%$,
e.g $60\%$ of indices is for reliable samples.
Then this mask is applied as dot product with noise signal.

We choose $\gamma = 0.1$ as mentioned in \ref{subsec:soft_thresh}.
Simulation is split to three variants: two conventional and one with deep learning.
Traditional are DRA \ref{alg:DRA} with same parameters except number of iterations: $50$ and $500$.
DRA with denoiser proposed in algorithm \ref{alg:pnp} is deep learned variant, we test $50$ iterations,
as in shorter conventional method.
In every algorithm is $\lambda$ scaled with coefficent $\alpha$ \todo{(MŠ: zvolil som alfu, ale v algoritme nie je vôbec. Je potrebné ju do algoritmov pridať?)} each iteration.
We set $\alpha$ to $0.9$ in algorithms with $50$ iterations and $1$ for DRA $500$ iterations, because the effect of $\lambda$ after $500$ divisions will be negligible.



\subsection{Results of tests}



%\todo{OM: Zde použít víc metrik, přinejmenším STOI, PESQ a SNR (možná vynechat SNR max a SNR gap). Dát 3 grafy vedle sebe pomocí figure*.}
\todo{MŠ-Graf sa ešte doplní lepší, musím vymyslieť ešte ako.}

\begin{figure*}
	\includegraphics[width=\linewidth]{figures/final2_with_errors}
	\caption{Široký obrázek.}
	\label{fig:final2witherrors}
\end{figure*}

\section{Conclusion}
\label{sec:conclusion}

\todo{Naše metoda je super (až na to, že nefunguje, ale to nebudeme říkat).}

\todo{Dalším postupem bude testovat použití na jiné související úlohy dle principu plug-and-play (declipping, dekvantizace). Nabízí se zkusit nepoužívat off-the-shelf denoiser, ale vlastní, který je učený na datech příbuzných s naší úlohou (např.\ specifický šum vzniklý výpadkem jistého procenta vzorků). Taktéž by se nabízel transfer learning, tj.\ vzít dobrý denoiser a jenom ho doučit, aby byl schopný redukovat artefakty vzniklé v inpaintovací úloze.}

\section*{Appendix: Derivation of the projection-like update}

\noindent
\todo{\textbf{(do článku bych asi nedával, spíš pro info})}
\todo{%
	Dle \cite[Tab.\,1]{Combettes2011}
	\begin{equation}
		\operatorname{prox}_f(\mathbf{y}) = (\mathbf{I} + \alpha \mathbf{M}_{\mathrm{R}}^\top\mathbf{M}_{\mathrm{R}})^{-1}(\mathbf{y} + \alpha\mathbf{M}_{\mathrm{R}}^\top\mathbf{M}_{\mathrm{R}}\mathbf{s}).
	\end{equation}
	Protože $\mathbf{M}_{\mathrm{R}}^\top\mathbf{M}_{\mathrm{R}}$ je diagonální matice, která má 1 na pozicích $I^R$ a 0 na pozicích $I^M$ diagonály, můžeme rozepsat po vzorcích (pozor, $n$ zde indexuje vzorky, ne iterace)
	\begin{equation}
		\left[\operatorname{prox}_f(\mathbf{y})\right]_n = \begin{cases}
			\frac{1}{1+\alpha}(y_n + \alpha s_n) \quad &\text{pro } n\in I^R, \\
			y_n\quad &\text{pro } n\in I^M. 
		\end{cases}
	\end{equation}
	V našem případě $(1-\beta)\mathbf{x} + \beta \operatorname{proj}_{\Gamma}(\mathbf{x})$ jsou pozice $I^M$ zřejmé, protože projekce tyto vzorky nemění, tzn.\ máme
	\begin{equation}
		(1-\beta) x_n + \beta x_n = x_n \text{ pro } n\in I^M.
	\end{equation}
	Pro vzorky v $I^R$ naopak projekce vrací hodnotu $s_n$, tudíž
	\begin{equation}
		(1-\beta) x_n + \beta s_n = \frac{1}{1+\alpha}(x_n + \alpha s_n)\text{ pro } n\in I^R.
	\end{equation}
	Odsud
	\begin{equation}
		\beta = \frac{\alpha}{1+\alpha} = 1 - \frac{1}{1+\alpha} \Longrightarrow \alpha = \frac{\beta}{1-\beta}.
	\end{equation}
}

%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{IEEEtr}
\bibliography{bib_eeict2023}



\end{document}
