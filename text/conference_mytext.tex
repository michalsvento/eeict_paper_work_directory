\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Audio restoration using plug-and-play approach\\
	{}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Michal Švento}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{ Brno University of Technology}\\
Brno, Czech Republic \\
212584@vut.cz}
\and
\IEEEauthorblockN{Ondřej Mokrý}
\IEEEauthorblockA{\textit{Signal Processing Laboratory} \\
\textit{ Brno University of Technology}\\
Brno, Czech Republic \\
xmokry12@vut.cz}

}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
speech enhancement, deep learning, Douglas-Rachford algorithm
\end{IEEEkeywords}

\section{Introduction}

Audio enhancement tasks mostly face problems like missing or damaged samples, noise, or clipping.
If we consider speech signal, we should not avoid the intelligibility problems.
Each problem has developed its own way of enhancing the signal.
Nowadays, the bestway to differentiate algorithms is with two categories:
conventional (autoregressive models, sparsity-based) and solutions using deep learning.

In conventional methods dominates Janssen \cite{Janssen1986} and Etter~\cite{Etter1996}.
These approaches are based on autoregressive signal modeling \cite{Mokry2020}.
Sparse signal representation has changed efficiency of restoration, mainly because increase of computing power.
The information hidden in frequency representation (using proper time-frequency analysis) is sparse, i.e. we do not need each spectral coefficient to repair the signal with improved subjective results.
The most advanced works using sparsity are \cite{Adler2012,Kitic2015,Zaviska2019, Mokry2019}.


Deep learning algorithms have also made their own progress in this area.
The most efficient neural network models are autoencoders,
recurrent neural networks (RNNs) and
Generative Adversial Network (GAN).
Current state-of-the-art deep learned algorithms are Speech Enhancement GAN (SEGAN) \cite{Pascual2017}, NSNet \cite{Xia2020}, FullSubNet \cite{Hao2021}. 

In \cite{Chan2016} was introduced Plug-and-Play method for image restoration.
The idea of a hybrid model,
combining conventional approach (convex minimization) with deep learning,
has shown succesful.
Our motivation is to transform this model to audio problems with minor differences.
We replace Alternating Direction Multiplier Method (ADMM) with Douglas-Rachford algorithm (DR~algorithm).
Denoiser will be chosen from state-of-the-art audio denoisers. 

%% Introduction to sections.
This paper is organized as follows. In section \ref{sec:prereq} we introduce our task in mathematical view and compose minimazation task.
Section \ref{sec:plugaandplay}  presents Plug-and-Play method and its challenges.
Section \ref{sec:eval} discusses about results and further improvements of algorithm.

\section{Prerequsities}\label{sec:prereq} 

In this section, we formulate the first task -- inpainting.
The proposed method \cite{Chan2016} assumes any damage,
but we start with missing samples and then expand the model for various damages.
The rest of the section explains minimization problem solved by DR algorithm.
Solution has two approaches.
First, using frequency coefficients as input explained in \ref{subsec:freqcoef}~\cite{Mokry2020}.
Second, using samples in time domain is described in subsection \ref{subsec:timecoef} \cite{Mokry2021}.


\subsection{Task formulation}

We consider column vector $ \mathbf{s} \in \mathbb{R}^{N} $ as our observed damaged single-channel signal of length $ N $.
We have set $ I $ of sample indices $ \{1,2,\dots,N\} $, which have two subsets: $ I^M $ for missing positions and $ I^R $ stands for reliable positions.
Therefore, samples $ \mathbf{s}(I^M) $ are considered reliable (undamaged) and $ \mathbf{s}(I^R) $ are samples, which we are looking for.
It is common to rewrite it in matrix form:

\begin{equation*}
	\mathbf{s}_{\mathrm{R}} = \mathbf{M}_{\mathrm{R}}\mathbf{s},
\end{equation*}
where $  M_{\mathrm{R}} $ is mask matrix $\mathbf{M}_{R} \in \mathbb{R} ^ { |I^R| \times N}$ ($ |I^R|$ as length of subset), selecting rows frow indentity matrix $ \mathbf{M} $\cite{Adler2012}.




\subsection{COEFFS INPUT Aprroach -- WORK NAME}\label{subsec:freqcoef}

We define our task as sparsity-based problem, minimizing $ \ell_0 $ norm of signal $ \mathbf{s} $.
However, this task leads to an NP-hard problem and is hardly solvable \cite{Mokry2020}.
The closest redefinition is to use the $ \ell_1 $ norm as follows:

\begin{equation*}
 \mathop {\operatorname{arg \, min}}_\mathbf {c}\Vert \mathbf {c}\Vert _1 \quad \text{s.t.}\ D\mathbf {c}\in \Gamma, \tag{1}
\end{equation*} 
where $ D $ is synthesis operator (inverse discrete Gabor transform) and we assume $ A $, $ D = A^* $ and therefore reconstructed signal corresponds to $ \mathbf {y} \approx  D\mathbf {c}$.
Set $ \Gamma $ is defined as follows:

\begin{equation*} \Gamma = \lbrace \mathbf {y}\in \mathbb {R}^L\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace, \tag{2}\end{equation*}

One of the suitable solutions is DR algorithm in \ref{alg:DRA_c} \cite{Mokry2020}.

\begin{algorithm}
	\caption{Douglas-Rachford algorithm -- model with frequency coefficients}
	\begin{algorithmic}[1]\label{alg:DRA_c}
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE  $ \gamma > 0 $, $ \delta  \in (0,1)$ ,
		
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{c}}_n=\operatorname{proj}_{\Gamma}(\mathbf{c}_n) $ 
		\STATE $ \mathbf{c}_{n+1} = \mathbf{c}_n + \lambda \left( \operatorname{soft}_{\gamma}\left(2\mathbf{\widetilde{c}}_n-\mathbf{c}_n \right)-\mathbf{\widetilde{c}}_n\right)$
		\ENDFOR
		\RETURN $D(\operatorname{proj}_{\Gamma}(\mathbf{c}_n))$ 
	\end{algorithmic} 
\end{algorithm}
Operator $ \operatorname{proj}_{\Gamma}(arg)$ is projection onto convex set $ \Gamma $ and $\operatorname{soft}_{\gamma}(arg)$ is soft thresholding operator.
Both are proximal operators.

The condition   $ \delta  \in (0,1)$ is strict for convergence of solution \cite{Combettes2010}.


\subsection{SAMPLES as INPUT APPROACH}\label{subsec:timecoef}

Second approach uses time domain samples as input.
It has mainly computational advice against first approach, in algorithm are less time-frequency transformations (e.g less multiplication operations per iteration).
Problem is, 
that we do not know proximal operator of $ \ell_1 $ norm after analysis, but we can use approximal operator \cite{Mokry2021}.

The main minimazation task reformulates as:
\begin{equation*}
	\mathop {\operatorname{arg \, min}}_\mathbf {s}\Vert \mathbf {s}\Vert _1 \quad \text{s.t.}\ \mathbf {s}\in \Gamma, \tag{3}
\end{equation*} 
and $ \Gamma $ is similarly as in  (eqref2) % Tags???????  
\begin{equation*}
	\Gamma = \lbrace \mathbf {s}\in \mathbb {R}^N\mid M_{\mathrm{R}}\mathbf {y}=M_{\mathrm{R}}\mathbf {s}\rbrace. \tag{4} 
\end{equation*}
Afterwards our algorithm resolves to following: 
\begin{algorithm}
	\caption{Douglas-Rachford algorithm -- model with time coefficients}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $ \lambda > 0 $, $ \gamma>0 $, $ \mathbf{x}_0 \in \mathbb{R}^{N} $
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{x}}_n=\operatorname{proj}_{\Gamma}(\mathbf{x}_n) $ 
		\STATE $ \mathbf{x}_{n+1} = \mathbf{x}_n + \lambda \left( D\left(\operatorname{soft}_{\gamma}\left(A\left(2\mathbf{\widetilde{x}}_n-\mathbf{x}_n\right) \right)\right) -\mathbf{\widetilde{x}}_n\right)$
		\ENDFOR
		\RETURN $\operatorname{proj}_{\Gamma}(\mathbf{x}_n)$ 
	\end{algorithmic} 
\end{algorithm}

Projection, in this case, is simply replacing reconstructed samples in positions considered reliable.

\section{Plug-and-Play inpainting} \label{sec:plugaandplay}

\subsection{general algorithm}

\begin{algorithm}
	\caption{Plug-and-Play DR algorithm}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE in
		\FOR {$n = 0, 1, \dots$}
		\STATE $\mathbf{\widetilde{x}}_n=\operatorname{proj}_{\Gamma}(\mathbf{x}_n) $ 
		\STATE $ \mathbf{x}_{n+1} = \mathbf{x}_n + \lambda \left( \mathcal{D} \left(2\mathbf{\widetilde{x}}_n-\mathbf{x}_n \right)-\mathbf{\widetilde{x}}_n\right)$
		\ENDFOR
		\RETURN $\operatorname{proj}_{\Gamma}(\mathbf{x}_n)$ 
	\end{algorithmic} 
\end{algorithm}
\subsection{choice of denoiser}

\subsection{Denoisers}

\section{Testing data and evaluation}\label{sec:eval}


\section{Conclusion}

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{IEEEtran}
\bibliography{bib_eeict2023}



\end{document}
